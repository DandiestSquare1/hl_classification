library(data.table)
dir()
DT <- fread('SwingHiLoData.csv')
head(DT,2)
target = DT$recordType
DT[, recordType:=NULL]
str(DT)
library(caret)
library(glmnet)
library(xgboost)
DT
head(names(DT))
X_train = data.matrix(DT)
warnings()
sum(is.na(X_train))
names(DT)
setnames(DT, names(DT), gsub(' ', '', names(DT)))
names(DT)
?regex
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
head(names(DT))
data.matrix(DT[1:100])
warnigns()
warnings()
sum(is.na(DT))
str(DT)
for(j in names(DT))
set(DT, j=j, value=ifelse(DT[[j]] == 'TRUE', 1, 0))
str(DT)
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
str(DT)
X_train = data.matrix(DT)
sum(is.na(X_train))
class_tbl = table(target)
class_tbl
scale_pos_weight = class_tbl[[1]] / class_tbl[[2]]
scale_pos_weight
dtrain <- xgb.DMatrix(X_train, label=log_cost, missing = NA)
dtest <- xgb.DMatrix(X_test, missing = NA)
target
tail(target)
shuffle = sample(1:nrow(DT), nrow(DT))
shuffle
DT = DT[shuffle]
X_train = data.matrix(DT)
class_tbl = table(target)
scale_pos_weight = class_tbl[[1]] / class_tbl[[2]]
set.seed(123)
shuffle = sample(1:nrow(DT), nrow(DT))
DT = DT[shuffle]
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
DT <- fread('SwingHiLoData.csv')
head(DT,2)
target = DT$recordType
DT[, recordType:=NULL]
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
shuffle = sample(1:nrow(DT), nrow(DT))
DT = DT[shuffle]
target = target[shuffle]
X_train = data.matrix(DT)
class_tbl = table(target)
scale_pos_weight = class_tbl[[1]] / class_tbl[[2]]
dtrain <- xgb.DMatrix(X_train, label=target, missing = NA)
dtest <- xgb.DMatrix(X_test, missing = NA)
par <- list(booster = "gbtree", objective = "reg:linear", eta = 0.2,
min_child_weight = 50, gamma = .7, subsample = 0.6, colsample_bytree = .6,
max_depth = 12, verbose = 1, scale_pos_weight = 1, lambda = 50, alpha=.1,
eval_metric = eval_metric)
par <- list(booster = "gbtree", objective = "reg:linear", eta = 0.2,
min_child_weight = 50, gamma = .7, subsample = 0.6, colsample_bytree = .6,
max_depth = 12, verbose = 1, scale_pos_weight = 1, lambda = 50, alpha=.1,
eval_metric = 'auc')
bst  <- xgb.cv(params = par, data = dtrain , nrounds = 300,
nfold = 5, verbose=TRUE)
dir()
rm(list = ls(all = TRUE)); gc()
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
DT <- fread('swingData_ES_1.50 (1).csv')
head(names(DT))
system('head -2 swingData_ES_1.50 (1).csv')
system('head -2 "swingData_ES_1.50 (1).csv"')
DT <- fread('swingData_ES_1.50 (1).csv', sep=',')
DT
DT <- fread('swingData_ES_1.50_v2.csv', sep=',')
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT)
DT
head(names(DT))
target = DT[, 1, with=F]$swingRecordType
DT[, swingRecordType:=NULL]
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
setnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
X_all = data.matrix(DT)
target = ifelse(target == -1, 2, target)
dtest <- xgb.DMatrix(X_all[1:20000, ], label=target[1:20000], missing = NA)
dtrain <- xgb.DMatrix(X_all[20001:nrow(X_all), ], label=target[20001:length(target)], missing = NA)
par <- list(objective = "multi:softmax", # multinomial
eval_metric = "mlogloss", # loss metric
num_class = 3, nthread = 4,
eta = 0.1, #learning rate
min_child_weight = 50, gamma = .7, # decision tree parameters
subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)
max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.
)
cv.nround = 50
bst.cv = xgb.cv(param=par, data = dtrain, label = target,
nfold = 5, nrounds=cv.nround)
dtest <- xgb.DMatrix(X_all[1:20000, ], label=target[1:20000], missing = NA)
dtrain <- xgb.DMatrix(X_all[20001:nrow(X_all), ], label=target[20001:length(target)], missing = NA)
par <- list(objective = "multi:softmax", # multinomial
eval_metric = "mlogloss", # loss metric
num_class = 3, nthread = 4,
eta = 0.1, #learning rate
min_child_weight = 50, gamma = .7, # decision tree parameters
subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)
max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.
)
par <- list(objective = "multi:softmax", # multinomial
eval_metric = "mlogloss", # loss metric
num_class = 3, nthread = 4,
eta = 0.2, #learning rate
min_child_weight = 50, gamma = .7, # decision tree parameters
subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)
max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.
)
cv.nround = 200
bst.cv = xgb.cv(param=par, data = dtrain, label = target,
nfold = 5, nrounds=cv.nround)
gdbt <- xgb.train(param=par, data=dtrain, nrounds=30)
xgb.dump(gdbt, fname='2xgboost.xgb', with.stats=TRUE)
names <- colnames(X_all)
importance_matrix <- xgb.importance(names, model = gdbt)
write.csv(importance_matrix, file='importance_matrix.txt', row.names = FALSE)
png('feature_importance.png', width = 400, height = 800)
xgb.plot.importance(importance_matrix[1:30,])
dev.off()
png('feature_importance2.png', width = 400, height = 800)
xgb.plot.importance(importance_matrix[31:60,])
dev.off()
save(importance_matrix, file='importance_gdbt.RData')
save(gdbt, file='2gdbt.RData')
yhat = predict(gdbt, dtest, missing = NA)
yhat = ifelse(yhat == 2, -1, yhat)
target = ifelse(target == 2, -1, target)
target
cfm = table(target=target[1:20000], yhat=yhat)
cfm
sum(diag(cfm)) / sum(cfm)
head(names(X_train))
names(DT)
grep('swingSize', names(DT))
DT[1:3, 1:10, wi=F]
fits <- lapply(seq(0, 1, 0.1), function(s){
cv.glmnet(X_all[20001:30000, ], y=target[20001:30000],
nfolds=3, type.measure = 'class', family="multinomial", alpha=s)
})
save(fits, file='elastic_net.RData')
glmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(cbind, cv_accuracy)))
cv_accuracy = lapply(fits, function(fit){
fit$cvm[fit$lambda == fit$lambda.1se]
})
cv_accuracy = lapply(fits, function(fit){
fit$cvm[fit$lambda == fit$lambda.1se]
})
cv_accuracy
do.call(cbind, cv_accuracy)
do.call(c, cv_accuracy)
glmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(c, cv_accuracy)))
glmnet_results
setnames(glmnet_results, names(glmnet_results), c('s', 'cv_accuracy'))
glmnet_results
glmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(c, cv_accuracy)))
setnames(glmnet_results, names(glmnet_results), c('s', 'cv_classification_error'))
cv_classification_error = lapply(fits, function(fit){
fit$cvm[fit$lambda == fit$lambda.1se]
})
glmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(c, cv_classification_error)))
setnames(glmnet_results, names(glmnet_results), c('s', 'cv_classification_error'))
glmnet_results
best_s = glmnet_results[max(cv_classification_error)]$s
best_s
glmnet_results[max(cv_classification_error)]
glmnet_results[min(cv_classification_error)]
glmnet_results
best_s = glmnet_results[which.min(cv_classification_error)]$s
best_s
glmnet_results[which.min(cv_classification_error)]
fit <- cv.glmnet(X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],
nfolds=3, type.measure = 'class', family="multinomial", alpha=best_s)
fit <- cv.glmnet(X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],
nfolds=10, type.measure = 'class', family="multinomial", alpha=best_s)
probs = predict(fit, newx=X_all[1:20000], s='lambda.1se')
head(probs)
str(probs)
plot(fit)
best_s
fit
probs = predict(fit, newx=X_all[1:20000], s='lambda.1se')
probs = predict(fit, newx=X_all[1:20000, ], s='lambda.1se')
head(probs)
probs = predict(fit, newx=X_all[1:20000, ], s='lambda.1se', type='response')
head(probs)
?predict.glmnet
probs = predict(fit, newx=X_all[1:20000, ], type='response', s='lambda.1se')
probs
str(probs)
names(probs)
head(probs)
probs[1:10, ]
probs[1:10]
probs
probs = data.table(probs)
probs
probs = predict(fit, newx=X_all[1:20000, ], type='response', s='lambda.1se')
probs = as.matrix(probs)
head(probs)
probs = predict(fit, newx=X_all[1:20000, ], type='response', s='lambda.1se')
probs
head(probs)
head(probs, 100)
class(probs)
probs[1, 1,1]
probs[1, 1:3,1]
probs[1, ,]
probs[, ,]
as.matrix(probs[, ,])
x = as.matrix(probs[, ,])
head(x)
probs = data.table(as.matrix(probs[, ,]))
p
probs
pred_inx = apply(probs, 1, function(x) which.max(x))
pred_inx
head(probs, 2)
preds = ifelse(pred_inx == 1, -1,
ifelse(pred_inx == 2, 0, 1))
preds
cfm_glmnet = table(target=target[1:20000], yhat=preds)
cfm_glmnet
sum(diag(cfm_glmnet))
sum(diag(cfm_glmnet)) / sum(cfm_glmnet)
sum(cfm_glmnet)
sum(diag(cfm_glmnet)) / sum(cfm_glmnet)
library(handy)
COEF(fit, s='lambda.1se')
fit
fit
coef_glm = predict(fit, newx=X_all[1:20000, ], type='coefficients', s='lambda.1se')
coef_glm
str(coef_glm)
as.matrix(coef_glm)
coef_glm
names(coef_glm)
vars_class_low = as.matrix(coef_glm[['-1']])
vars_class_low
vars_class_low[,1]
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
vars_class_low
names(coef_glm)
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
})
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
vars_class_low[sort(abs(vars_class_low), decreasing=TRUE)]
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
print(abs(vars_class_low))
vars_class_low[sort(abs(vars_class_low), decreasing=TRUE)]
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
print(abs(vars_class_low))
sort(abs(vars_class_low), decreasing=TRUE)
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
vars_class_low[order(abs(vars_class_low), decreasing=TRUE)]
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
coefs
cfm_glmnet
xgb.plot.importance(importance_matrix[1:30,])
xgb.plot.importance(importance_matrix[31:60,])
coefs
?train
target[20001:nrow(X_all)]
nnet = train(x=X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],
method = 'nnet', metric = 'Accuracy',
maximize = TRUE, tuneLength = 10)
nnet = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nnet', metric = 'Accuracy',
maximize = TRUE, tuneLength = 10)
# It has the advantage that is fast to train On2 and selects the "best features".
X_all = cbind(target, X_all)
namespaces
warnings()
nnet
trainControl()
head(X_all, 2)
X_all[,1:100]
X_all[2,1:100]
X_all[1:2,1:100]
X_all[1:2,1:50]
X_all[1:2,1:60]
X_all[1:2,1:80]
X_all[1:2,1:90]
?trainControl
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
# It's a nice line to have in the start of the script, so it's reproducible.
# gc() frees memory. Really important with large datasets
# install.packages(c('devtools','data.table', 'caret', 'glmnet'))
# devtools::install_github('dmlc/xgboost',subdir='R-package')
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
# Read data
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
DT
DT[1]
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
# It's a nice line to have in the start of the script, so it's reproducible.
# gc() frees memory. Really important with large datasets
# install.packages(c('devtools','data.table', 'caret', 'glmnet'))
# devtools::install_github('dmlc/xgboost',subdir='R-package')
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
# Read data
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
# separate the target from the dataset
target = DT[, 1, with=F]$swingRecordType
DT[, swingRecordType:=NULL] # remove the column from the training data
# Remove spaces from variable names
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
# This just removes symbols from the variable names
setnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))
# see ?regex for more examples
# data.table allows you to loop (really really fast) over each column
# and is very memory efficient. It changes each column without having an extra copy of the data.
# This is really relevant when working with large datasets
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
#shuffle = sample(1:nrow(DT), nrow(DT))
#DT = DT[shuffle]
#target = target[shuffle]
### Cross-validation
X_all = data.matrix(DT)
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
