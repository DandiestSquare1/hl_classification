})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
get_not_null_coef <- function(coef_glm){
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
vars_class_low[order(abs(vars_class_low), decreasing=TRUE)]
})
names(cf) <- classes
cf
}
coefs = get_not_null_coef(coef_glm)
coefs
coefs
cfm_glmnet
xgb.plot.importance(importance_matrix[1:30,])
xgb.plot.importance(importance_matrix[31:60,])
coefs
?train
target[20001:nrow(X_all)]
nnet = train(x=X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],
method = 'nnet', metric = 'Accuracy',
maximize = TRUE, tuneLength = 10)
nnet = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nnet', metric = 'Accuracy',
maximize = TRUE, tuneLength = 10)
# It has the advantage that is fast to train On2 and selects the "best features".
X_all = cbind(target, X_all)
namespaces
warnings()
nnet
trainControl()
head(X_all, 2)
X_all[,1:100]
X_all[2,1:100]
X_all[1:2,1:100]
X_all[1:2,1:50]
X_all[1:2,1:60]
X_all[1:2,1:80]
X_all[1:2,1:90]
?trainControl
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
# It's a nice line to have in the start of the script, so it's reproducible.
# gc() frees memory. Really important with large datasets
# install.packages(c('devtools','data.table', 'caret', 'glmnet'))
# devtools::install_github('dmlc/xgboost',subdir='R-package')
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
# Read data
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
DT
DT[1]
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
# It's a nice line to have in the start of the script, so it's reproducible.
# gc() frees memory. Really important with large datasets
# install.packages(c('devtools','data.table', 'caret', 'glmnet'))
# devtools::install_github('dmlc/xgboost',subdir='R-package')
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
# Read data
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
# separate the target from the dataset
target = DT[, 1, with=F]$swingRecordType
DT[, swingRecordType:=NULL] # remove the column from the training data
# Remove spaces from variable names
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
# This just removes symbols from the variable names
setnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))
# see ?regex for more examples
# data.table allows you to loop (really really fast) over each column
# and is very memory efficient. It changes each column without having an extra copy of the data.
# This is really relevant when working with large datasets
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
#shuffle = sample(1:nrow(DT), nrow(DT))
#DT = DT[shuffle]
#target = target[shuffle]
### Cross-validation
X_all = data.matrix(DT)
nb = train(x=X_all[20001:nrow(X_all), ], y=as.factor(target[20001:nrow(X_all)]),
method = 'nb', metric = 'Accuracy',
trControl = trainControl(method = "cv", number=5, repeats=1),
maximize = TRUE, tuneLength = 10)
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
# Read data
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
# separate the target from the dataset
target = DT[, 1, with=F]$swingRecordType
DT[, swingRecordType:=NULL] # remove the column from the training data
# Remove spaces from variable names
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
# This just removes symbols from the variable names
setnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))
# see ?regex for more examples
# data.table allows you to loop (really really fast) over each column
# and is very memory efficient. It changes each column without having an extra copy of the data.
# This is really relevant when working with large datasets
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
#shuffle = sample(1:nrow(DT), nrow(DT))
#DT = DT[shuffle]
#target = target[shuffle]
### Cross-validation
X_all = data.matrix(DT)
# selecting number of Rounds
target = ifelse(target == -1, 2, target) # xgboost only accepts class numbers between [0, positive numbers)
dtest <- xgb.DMatrix(X_all[1:20000, ], label=target[1:20000], missing = NA)
dtrain <- xgb.DMatrix(X_all[20001:nrow(X_all), ], label=target[20001:length(target)], missing = NA)
# dtest <- xgb.DMatrix(X_test, missing = NA)
# Set parameters for xgboost model
par <- list(objective = "multi:softmax", # multinomial
eval_metric = "mlogloss", # loss metric
num_class = 3, nthread = 4,
eta = 0.2, #learning rate
min_child_weight = 50, gamma = .7, # decision tree parameters
subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)
max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.
)
### Run Cross Valication ###
# to figure out when the model starts overfitting
# See cvlog.txt. Around 50 trees, the train multinomial log loss function starts
# separating from the testing set.
cv.nround = 200
bst.cv = xgb.cv(param=par, data = dtrain, label = target,
nfold = 5, nrounds=cv.nround)
install.packages("entropy")
install.packages("sda")
install.packages("corpcor")
install.packages("fdrtool")
install.packages("lda")
install.packages("vioplot")
file.to.open <- file.choose()
raw.data <- read.csv(file.to.open, header = T)
file.to.open <- file.choose()
raw.data <- read.csv(file.to.open, header = T)
head(raw.data)
source("constants.R")
file.to.open
homedir <-"~/Desktop/dm/free/09_september/hl_classification/"
setwd(homedir)
source("scritps/constants.R")
homedir <-"~/Desktop/dm/free/09_september/hl_classification/"
setwd(homedir)
getwd()
dir()
source("scritps/constants.R")
getwd()
source("/scritps/constants.R")
dir()
list.files('/scripts')
list.files('scripts')
source("scritps/constants.R")
source("../scritps/constants.R")
source("../../scritps/constants.R")
source("/scritps/constants.R")
source("constants.R")
rm(list = ls(all = TRUE)); gc() # this removes all the elements from the global environment
library(data.table)
library(caret)
library(glmnet)
library(xgboost)
DT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')
DT <- setDT(DT) # converts the data.frame to data.table
target = DT[, 1, with=F]$swingRecordType
DT[, swingRecordType:=NULL] # remove the column from the training data
setnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))
setnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))
for(j in names(DT))
set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))
set.seed(123)
X_all = data.matrix(DT)
target = ifelse(target == -1, 2, target) # xgboost only accepts class numbers between [0, positive numbers)
dtest <- xgb.DMatrix(X_all[1:20000, ], label=target[1:20000], missing = NA)
dtrain <- xgb.DMatrix(X_all[20001:nrow(X_all), ], label=target[20001:length(target)], missing = NA)
par <- list(objective = "multi:softmax", # multinomial
eval_metric = "mlogloss", # loss metric
num_class = 3, nthread = 4,
eta = 0.2, #learning rate
min_child_weight = 50, gamma = .7, # decision tree parameters
subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)
max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.
)
cv.nround = 200
bst.cv = xgb.cv(param=par, data = dtrain, label = target,
nfold = 5, nrounds=cv.nround)
gdbt <- xgb.train(param=par, data=dtrain, nrounds=30)
xgb.dump(gdbt, fname='../models/2xgboost.xgb', with.stats=TRUE)
names <- colnames(X_all)
importance_matrix <- xgb.importance(names, model = gdbt)
write.csv(importance_matrix, file='importance_matrix.txt', row.names = FALSE)
gdbt
getwd()
xgb.dump(gdbt, fname='models/2xgboost.xgb', with.stats=TRUE)
write.csv(importance_matrix, file='results/xgboost_importance_matrix.txt', row.names = FALSE)
head(importance_matrix)
importance_matrix
png('results_XGBoost/best_30_feature_importance.png', width = 400, height = 1000)
xgb.plot.importance(importance_matrix[1:30,])
dev.off()
png('results_XGBoost/31-60_feature_importance.png', width = 400, height = 1000)
xgb.plot.importance(importance_matrix[31:60,])
dev.off()
write.csv(importance_matrix, file='results_XGBoost/xgboost_importance_matrix.txt', row.names = FALSE)
source('~/Desktop/dm/free/09_september/hl_classification/martin_scripts/XGBoost.R', echo=TRUE)
sum(diag(cfm)) / sum(cfm)
ls()
save.image(file = 'results_XGBoost/XGBoost_image.RData')
rm(list = ls(all = TRUE)); gc()
load('results_XGBoost/XGBoost_image.RData')
importance_matrix
importance_matrix[order(-Gain)]
importance_matrix[order(-Gain)][1:10]
importance_matrix[order(-Gain)][1:20]
quantile(importance_matrix$Gain, seq(0, 1, 0.1)
)
quantile(importance_matrix, 0.7)
quantile(importance_matrix$Gain, 0.7)
quantile(importance_matrix$Gain, 0.7)[[1]]
threshold = quantile(importance_matrix$Gain, 0.7)[[1]] # keep the 30% top features
importance_matrix[Gain > threshold]
importance_matrix[Gain >= threshold]
importance_matrix = importance_matrix[Gain >= threshold]
importance_matrix
library(caret)
featurePlot
importance_matrix
ggplot(importance_matrix, aes(Gain, feature)) +
geom_point()
ggplot(importance_matrix, aes(Gain, Feature)) +
geom_point()
importance_matrix = importance_matrix[order(-Gain)]
ggplot(importance_matrix, aes(Gain, Feature)) +
geom_point()
importance_matrix = importance_matrix[, Feature:=factor(Feature, levels=c(sort(unique(Feature))))]
importance_matrix
ggplot(importance_matrix, aes(Gain, Feature)) +
geom_point()
importance_matrix = importance_matrix[, Feature:=factor(Feature, levels=Feature[order(Gain)])]
importance_matrix
ggplot(importance_matrix, aes(Gain, Feature)) +
geom_point()
p = ggplot(importance_matrix, aes(Gain, Feature)) +
geom_point() +
theme_bw() +
labs(list(title = 'Best 30% Features, sorted by the importance metric (Gain)'))
png('results_XGBoost/Best_30%_feature_importance.png', width = 400, height = 1000)
print(p)
dev.off()
importance_matrix
importance_matrix$Feature
best_features = as.character(importance_matrix$Feature)
best_features
DT
DT[, best_features]
DT[, best_features, with=FALSE]
target
data = cbind(target, DT[, best_features, with=FALSE])
data
data[, lapply(.SD, function(x) mean(x, na.rm=TRUE)), by=target, .SDcols=best_features]
mean_by_target = data[, lapply(.SD, function(x) mean(x, na.rm=TRUE)), by=target, .SDcols=best_features]
t(mean_by_target)
mean_by_target
cat('Means of the best feature by each target\n')
print(mean_by_target)
source('~/Desktop/dm/free/09_september/hl_classification/XGBoost_report.R', echo=TRUE)
getwd()
png('results_XGBoost/Best_30%_feature_importance.png', width = 400, height = 1000)
print(p)
png('results_XGBoost/Best_30_percent_feature_importance.png', width = 400, height = 1000)
print(p)
dev.off()
source('~/Desktop/dm/free/09_september/hl_classification/XGBoost_report.R', echo=TRUE)
scale(mean_by_target)
?scale
scale(mean_by_target, center=TRUE, scale=FALSE)
as.matrix(scale(mean_by_target, center=TRUE, scale=FALSE))
names(scale(mean_by_target, center=TRUE, scale=FALSE))
str(scale(mean_by_target, center=TRUE, scale=FALSE))
mean_by_target_centered = scale(mean_by_target, center=TRUE, scale=FALSE)
mean_by_target_centered
data.frame(mean_by_target_centered)
mean_by_target_centered = data.frame(mean_by_target_centered)
cat('Means of the best feature by each target, centered around the mean\n')
print(mean_by_target_centered)
plot(mean_by_target_centered)
melt(mean_by_target_centered)
library(reshape2)
melt(mean_by_target_centered)
head(melt(mean_by_target_centered))
head(melt(mean_by_target_centered))
head(mean_by_target_centered)
melt(mean_by_target_centered, id.vars='target')
mdf = melt(mean_by_target_centered, id.vars='target')
head(mdf)
ggplot(mdf, aes(variable, value, color=target, group=target)) +
geom_point()
ggplot(mdf, aes(variable, value, color=target, group=target)) +
geom_line() +
theme_bw()
ggplot(mdf, aes(variable, value, color=target, group=target)) +
geom_line() +
theme_bw() +
facet_wrap(~variable)
ggplot(mdf, aes(variable, value, color=target)) +
geom_point(size=4) +
theme_bw() +
facet_wrap(~variable)
ggplot(mdf, aes(variable, value, color=target)) +
geom_point(size=4) +
theme_bw()
ggplot(mdf, aes(variable, value, color=target)) +
geom_text(size=4) +
theme_bw()
ggplot(mdf, aes(variable, value, color=target)) +
geom_text(label=target, size=4) +
theme_bw()
ggplot(mdf, aes(variable, value, color=target)) +
geom_text(aes(label=target)) +
theme_bw()
p = ggplot(mdf, aes(variable, value, color=target)) +
geom_text(aes(label=target)) +
theme_bw() +
labs(list(title='Means of best features by each target, centered around the mean',
x = 'Variables', y='Mean by target as define by the labels'))
png('results_XGBoost/mean_by_class_centered.png', width = 400, height = 1000)
print(p)
dev.off()
sapply(pkgs, require, character.only=TRUE)
pkgs = c('data.table', 'caret', 'glmnet', 'xgboost', 'reshape2',
'ggplot2')
sapply(pkgs, require, character.only=TRUE)
rm(list = ls(all = TRUE))
source('load_packages.R')
rm(list = ls(all = TRUE))
source('martin_scripts/load_packages.R')
load('results_XGBoost/XGBoost_image.RData') # gets data and models from XGBoost.R
fits <- lapply(seq(0, 1, 0.1), function(s){
cv.glmnet(X_all[20001:30000, ], y=target[20001:30000],
nfolds=3, type.measure = 'class', family="multinomial", alpha=s)
})
load('elastic_net.RData')
dir()
load('results_regression/elastic_net.RData')
cv_classification_error = lapply(fits, function(fit){
fit$cvm[fit$lambda == fit$lambda.1se]
})
cv_classification_error
glmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(c, cv_classification_error)))
glmnet_results
setnames(glmnet_results, names(glmnet_results), c('s', 'cv_classification_error'))
glmnet_results
best_s = glmnet_results[which.min(cv_classification_error)]$s # get the best s, from the previous models
best_s
fit <- cv.glmnet(X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],
nfolds=10, type.measure = 'class', family="multinomial", alpha=best_s)
plot(fit)
save(fit, file='results_regression/best_regression.RData')
probs = predict(fit, newx=X_all[1:20000, ], type='response', s='lambda.1se')
probs = data.table(as.matrix(probs[, ,]))
pred_inx = apply(probs, 1, function(x) which.max(x))
preds = ifelse(pred_inx == 1, -1,
ifelse(pred_inx == 2, 0, 1))
cfm_glmnet = table(target=target[1:20000], yhat=preds)
cfm_glmnet
sum(diag(cfm_glmnet)) # total of elements in the diagonal of the confusion matrix (correctly classified)
/
sum(cfm_glmnet) # total of elements
sum(diag(cfm_glmnet)) # total of elements in the diagonal of the confusion matrix (correctly classified)
/
sum(cfm_glmnet) # total of elements
sum(diag(cfm_glmnet)) / sum(cfm_glmnet) # total of elements
coef_glm = predict(fit, newx=X_all[1:20000, ], type='coefficients', s='lambda.1se')
# This function just makes taking a look at the coefficients easier.
get_not_null_coef <- function(coef_glm){
# Orders coefficients of each class by the absolute value of each coefficient
classes = names(coef_glm)
cf = lapply(classes, function(x){
vars_class_low = as.matrix(coef_glm[[x]])
vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]
vars_class_low[order(abs(vars_class_low), decreasing=TRUE)]
})
names(cf) <- classes
cf
}
# coefs has the value of not-null coefficients
# ranked by absolute value.
coefs = get_not_null_coef(coef_glm)
coefs
save(coefs, file='results_regression/feature_importance.RData')
save.image('results_regression/regression_image.RData')
rm(list = ls(all = TRUE)); gc()
source('load_packages.R')
source('martin_scripts/load_packages.R')
load('results_regression/regression_image.RData')
coefs
coefs[1]
ldply(cbind, coefs[1])
source('martin_scripts/load_packages.R')
ldply(cbind, coefs[1])
coefs[1]
cbind(names(coefs[1]), coefs[[1]])
coefs[[1]]
cbind(names(coefs[1]), coef=a.vector(coefs[[1]]))
cbind(names(coefs[1]), coef=as.vector(coefs[[1]]))
names(coefs[1])
names(coefs[[1]])
coefs[1]
str(coefs[1])
names(coefs[1])
names(coefs[[1]])
cbind(names(coefs[1]), feature=names(coefs[[1]]), coef=as.vector(coefs[[1]]))
coef_-1 = data.table(cbind(names(coefs[1]), feature=names(coefs[[1]]), coef=as.vector(coefs[[1]])))
coef_neg1 = data.table(cbind(names(coefs[1]), feature=names(coefs[[1]]), coef=as.vector(coefs[[1]])))
coef_neg1
coef_neg1 = data.table(cbind(class=names(coefs[1]), feature=names(coefs[[1]]), coef=as.vector(coefs[[1]])))
coef_neg1
head(coef_neg1)
coef_table = list()
coef_table = list()
for(i in 1:3){
coef_table[[i]] = data.table(cbind(class=names(coefs[1]),
feature=names(coefs[[1]]),
coef=as.vector(coefs[[1]])))
}
coef_table
coef_table = list()
for(i in 1:3){
coef_table[[i]] = data.table(cbind(class=names(coefs[i]),
feature=names(coefs[[i]]),
coef=as.vector(coefs[[i]])))
}
coef_table
get_best_features = lapply(coef_table, function(x){
threshold = 0.7
cut = quantile(abs(x$coef), threshold)
x[abs(coef) > cut]
})
get_best_features = lapply(coef_table, function(x){
threshold = 0.7
cut = quantile(abs(x$coef), probs=threshold)
x[abs(coef) > cut]
})
get_best_features = lapply(coef_table, function(x){
threshold = 0.7
x[, coef:=as.numeric(coef)]
cut = quantile(abs(x$coef), probs=threshold)
x[abs(coef) > cut]
})
get_best_features
get_best_features = lapply(coef_table, function(x){
threshold = 0.7
x[, coef:=as.numeric(coef)]
cut = quantile(abs(x$coef), probs=threshold)
x = x[abs(coef) > cut]
x[['feature']][-1]
})
get_best_features
unique(get_best_features)
unique(unlist(get_best_features))
best_features = unique(unlist(get_best_features))
best_features
data
DT
data = cbind(target, DT[, best_features, with=FALSE])
data
means_by_target = data[, lapply(.SD, function(x) mean(x, na.rm=TRUE)),
by=target, .SDcols=best_features]
means_by_target
mean_by_target_centered = scale(mean_by_target, center=TRUE, scale=FALSE)
mean_by_target = data[, lapply(.SD, function(x) mean(x, na.rm=TRUE)),
by=target, .SDcols=best_features]
mean_by_target_centered = scale(mean_by_target, center=TRUE, scale=FALSE)
mean_by_target_centered = data.frame(mean_by_target_centered)
mean_by_target_centered
mdf = melt(mean_by_target_centered, id.vars='target')
mdf
p = ggplot(mdf, aes(variable, value, color=target)) +
geom_text(aes(label=target)) +
theme_bw() +
labs(list(title='Means of best features by each target, centered around the mean',
x = 'Variables', y='Mean by target as define by the labels'))
p
png('results_regression/mean_by_class_centered.png', width = 400, height = 1000)
print(p)
dev.off()
