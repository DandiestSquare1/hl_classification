{
    "contents" : "rm(list = ls(all = TRUE)); gc()\n\nlibrary(data.table)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(xgboost)\n\n# to install \n# devtools::install_github('martinbel/handy')\nlibrary(handy) \n\nDT <- read.csv('swingData_ES_1.50_v2.csv', sep=',')\nDT <- setDT(DT)\n\n# exclude swingSize\n\ntarget = DT[, 1, with=F]$swingRecordType\nDT[, swingRecordType:=NULL]\n\nsetnames(DT, names(DT), gsub('[[:space:]]', '', names(DT)))\nsetnames(DT, names(DT), gsub('[[:punct:]]{1,}', '_', names(DT)))\n\nfor(j in names(DT))\n  set(DT, j=j, value=ifelse(grepl('TRUE', DT[[j]]), 1, 0))\n\nset.seed(123)\n#shuffle = sample(1:nrow(DT), nrow(DT))\n#DT = DT[shuffle]\n#target = target[shuffle]\n\n### Cross-validation\nX_all = data.matrix(DT)\n\n# selecting number of Rounds\ntarget = ifelse(target == -1, 2, target) # xgboost only accepts class numbers between [0, positive numbers)\ndtest <- xgb.DMatrix(X_all[1:20000, ], label=target[1:20000], missing = NA)\ndtrain <- xgb.DMatrix(X_all[20001:nrow(X_all), ], label=target[20001:length(target)], missing = NA)\n# dtest <- xgb.DMatrix(X_test, missing = NA)\n\n# Set parameters for xgboost model\npar <- list(objective = \"multi:softmax\", # multinomial \n            eval_metric = \"mlogloss\", # loss metric\n            num_class = 3, nthread = 4, \n            eta = 0.2, #learning rate\n            min_child_weight = 50, gamma = .7, # decision tree parameters\n            subsample = 0.6, colsample_bytree = .6, # prevent overfitting - random forest style (column and row sampling)\n            max_depth = 9 # tree maximum depth, controls overfitting. 9 is fairly conservative.\n)\n\n### Run Cross Valication ###\n# to figure out when the model starts overfitting\n# See cvlog.txt. Around 50 trees, the train multinomial log loss function starts \n# separating from the testing set.\ncv.nround = 200\nbst.cv = xgb.cv(param=par, data = dtrain, label = target, \n                nfold = 5, nrounds=cv.nround)\n\n### using a train-test split ###\ngdbt <- xgb.train(param=par, data=dtrain, nrounds=30)\nxgb.dump(gdbt, fname='2xgboost.xgb', with.stats=TRUE)\n\nnames <- colnames(X_all)\nimportance_matrix <- xgb.importance(names, model = gdbt)\nwrite.csv(importance_matrix, file='importance_matrix.txt', row.names = FALSE)\n\npng('feature_importance.png', width = 400, height = 800)\nxgb.plot.importance(importance_matrix[1:30,])\ndev.off()\n\npng('feature_importance2.png', width = 400, height = 800)\nxgb.plot.importance(importance_matrix[31:60,])\ndev.off()\n\n# Importance_matrix has a table with the gain of each feature\n# This is the \"importance metric\" used by the model\nsave(importance_matrix, file='importance_gdbt.RData')\nsave(gdbt, file='2gdbt.RData') # saves the model object as an RData file\n\n# testing predictions\nyhat = predict(gdbt, dtest, missing = NA)\nyhat = ifelse(yhat == 2, -1, yhat)\ntarget = ifelse(target == 2, -1, target)\n\n# confusion matrix\ncfm = table(target=target[1:20000], yhat=yhat)\n#           yhat\n# target      -1     0     1\n#      -1    749   710     0\n#       0    157 16938   146\n#       1      0   631   669\n\n# Accuracy \nsum(diag(cfm)) / sum(cfm)\n# [1] 0.9178\n\n### Blend with regularized logistic regression - LASSO\n# statweb.stanford.edu/~tibs/lasso/lasso.pdf\n# GLMNET R package introduction: \n# http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html\n\n\n# create a model matrix\nfits <- lapply(seq(0, 1, 0.1), function(s){\n  cv.glmnet(X_all[20001:30000, ], y=target[20001:30000],\n            nfolds=3, type.measure = 'class', family=\"multinomial\", alpha=s)\n})\nsave(fits, file='elastic_net.RData')\n\ncv_classification_error = lapply(fits, function(fit){\n  fit$cvm[fit$lambda == fit$lambda.1se]\n})\n\nglmnet_results = data.table(cbind(seq(0, 1, 0.1), do.call(c, cv_classification_error)))\nsetnames(glmnet_results, names(glmnet_results), c('s', 'cv_classification_error'))\n\n# Fit model with all data and best regularization parameter\nbest_s = glmnet_results[which.min(cv_classification_error)]$s\nfit <- cv.glmnet(X_all[20001:nrow(X_all), ], y=target[20001:nrow(X_all)],\n                 nfolds=10, type.measure = 'class', family=\"multinomial\", alpha=best_s)\n\n# Evaluate results in left out data\nprobs = predict(fit, newx=X_all[1:20000, ], type='response', s='lambda.1se')\nprobs = data.table(as.matrix(probs[, ,]))\npred_inx = apply(probs, 1, function(x) which.max(x))\npreds = ifelse(pred_inx == 1, -1,\n               ifelse(pred_inx == 2, 0, 1))\n\ncfm_glmnet = table(target=target[1:20000], yhat=preds)\n#           yhat\n# target      -1     0     1\n#      -1    717   742     0\n#       0    185 16906   150\n#       1      0   664   636\n\n# accuracy\nsum(diag(cfm_glmnet)) / sum(cfm_glmnet)\n# 0.91295\n\n# check best features in the regression model\ncoef_glm = predict(fit, newx=X_all[1:20000, ], type='coefficients', s='lambda.1se')\n\nget_not_null_coef <- function(coef_glm){\n  # Orders coefficients of each class by the absolute value of each coefficient\n  classes = names(coef_glm)\n  cf = lapply(classes, function(x){\n    vars_class_low = as.matrix(coef_glm[[x]])\n    vars_class_low = vars_class_low[vars_class_low[,1] != 0, ]\n    vars_class_low[order(abs(vars_class_low), decreasing=TRUE)]\n  })\n  names(cf) <- classes\n  cf\n}\n\ncoefs = get_not_null_coef(coef_glm)\ncoefs\n\n\n\n\n",
    "created" : 1441424408816.000,
    "dirty" : false,
    "encoding" : "LATIN1",
    "folds" : "",
    "hash" : "3584474440",
    "id" : "8C7BF776",
    "lastKnownWriteTime" : 1441503802,
    "path" : "~/Desktop/dm/free/09_september/hl_classification/2xgboost.R",
    "project_path" : "2xgboost.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}